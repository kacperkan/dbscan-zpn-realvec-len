{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import scipy as sp\n",
    "import scipy.stats as sp_stats\n",
    "import math\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "from functional import seq\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "})\n",
    "\n",
    "NUM_FEATURES_PER_DATASET = {\n",
    "    \"cluto\": 2,\n",
    "    \"complex9\": 2,\n",
    "    \"letter\": 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(Path(\"../experiments/\").rglob(\"STAT\"))\n",
    "len(files)\n",
    "metrics_columns = [\"Purity\", \"RAND\", \"Silhouette\", \"Davis-Bouldin\", \"\\#Multi Borders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_assignments(file_path: Path) -> List[List[int]]:\n",
    "    data = file_path.read_text().split(\"\\n\")\n",
    "    dataset = file.parent.name.split(\"_\")[0]\n",
    "    dataset = dataset if \"cluto\" not in dataset else \"cluto\"\n",
    "    output = []\n",
    "    for line in data:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        components = line.split(\",\")\n",
    "        class_assignments = seq(components[3 + NUM_FEATURES_PER_DATASET[dataset]:]).map(int).list()\n",
    "        output.append(class_assignments)\n",
    "    return output\n",
    "\n",
    "def get_num_border_points_with_multiple_assignments(path: Path) -> int:\n",
    "    return seq(get_class_assignments(path)).filter(lambda x: len(x) > 1).map(len).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in tqdm.tqdm(files):\n",
    "    if \"sanity_experiment\" in file.parent.name or \"final\" in file.parent.name:\n",
    "        print(\"Omitting: {}\".format(file.parent.name))\n",
    "        continue\n",
    "    with open(file) as f:\n",
    "        dataset, eps, min_pts, algo, _, filtering = file.parent.name.split(\"_\")\n",
    "        \n",
    "        sample = yaml.safe_load(f.read().replace(\"\\t\", \"\"))\n",
    "        \n",
    "        sample.update({\n",
    "            \"dataset\": dataset if \"cluto\" not in dataset else \"cluto\",\n",
    "            \"algorithm\": algo,\n",
    "            \"filtering\": filtering,\n",
    "            \"model_fitting\": sample[\"-model_fitting\"],\n",
    "            \"data_reading\": sample[\"-data_reading\"],\n",
    "            \"davis_bouldin\": float(sample[\"davis_bouldin\"]),\n",
    "        })\n",
    "        \n",
    "        class_assignments = get_class_assignments(file.parent / \"OUT\")\n",
    "        total_border_points = seq(class_assignments).filter(lambda x: len(x) > 1).map(len).sum()\n",
    "        \n",
    "        sample[\"\\#Multi Borders\"] = total_border_points\n",
    "        \n",
    "        \n",
    "        del sample[\"-model_fitting\"]\n",
    "        del sample[\"-data_reading\"]\n",
    "        del sample[\"runtimes_miliseconds\"]\n",
    "        del sample[\"input_file\"]\n",
    "        data.append(sample)\n",
    "        \n",
    "        \n",
    "frame = pd.DataFrame(data)\n",
    "frame = frame.rename(\n",
    "    columns={\n",
    "        'purity': 'Purity',\n",
    "        \"davis_bouldin\": \"Davis-Bouldin\",\n",
    "        \"rand\": \"RAND\",\n",
    "        \"silhouette\": \"Silhouette\"\n",
    "    }\n",
    ")\n",
    "frame = frame.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon, cluto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eps_for_dataset(dataset):\n",
    "    data = frame[\n",
    "        (frame[\"filtering\"] == \"none\")\n",
    "        & (frame[\"minPts\"] == 5)\n",
    "        & (frame[\"dataset\"] == dataset)\n",
    "    ]\n",
    "    data = pd.merge(\n",
    "        data[data[\"algorithm\"] == \"base\"], \n",
    "        data[data[\"algorithm\"] == \"tanimoto\"],\n",
    "        on=\"Eps\",\n",
    "        suffixes=[\"_base\", \"_tanimoto\"]\n",
    "    )\n",
    "    data = data.sort_values(by=\"Eps\")[\n",
    "        [\"Eps\"] + seq(\n",
    "            [[m + \"_base\", m + \"_tanimoto\"] for m in metrics_columns]\n",
    "        ).flatten().list()\n",
    "    ]\n",
    "    max_values = data.max()\n",
    "    for column in data.columns:\n",
    "        data[column] = data[column].apply(\n",
    "            lambda x: \"\\textbf{{{:.4f}}}\".format(x) if x == max_values[column] else \"{:.4f}\".format(x)\n",
    "        )\n",
    "    print(data.to_latex(escape=False, index=False))\n",
    "print_eps_for_dataset(\"cluto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon, complex9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_eps_for_dataset(\"complex9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon, letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_eps_for_dataset(\"letter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min pts, cluto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_min_pts_for_dataset(dataset):\n",
    "    data = frame[\n",
    "        (frame[\"filtering\"] == \"none\")\n",
    "        & (\n",
    "            (\n",
    "                (frame[\"Eps\"] == 0.1) & (frame[\"algorithm\"] == \"base\")\n",
    "                | (frame[\"Eps\"] == 0.99) & (frame[\"algorithm\"] == \"tanimoto\")\n",
    "            )\n",
    "        )\n",
    "        & (frame[\"dataset\"] == dataset)\n",
    "    ]\n",
    "    data = pd.merge(\n",
    "        data[data[\"algorithm\"] == \"base\"], \n",
    "        data[data[\"algorithm\"] == \"tanimoto\"],\n",
    "        on=\"minPts\",\n",
    "        suffixes=[\"_base\", \"_tanimoto\"]\n",
    "    )\n",
    "    data = data.sort_values(by=\"minPts\")[\n",
    "        [\"minPts\"] + seq(\n",
    "            [[m + \"_base\", m + \"_tanimoto\"] for m in metrics_columns]\n",
    "        ).flatten().list()\n",
    "    ]\n",
    "    max_values = data.max()\n",
    "    print(len(data))\n",
    "    for column in data.columns:\n",
    "        data[column] = data[column].apply(\n",
    "            lambda x: \"\\textbf{{{:.4f}}}\".format(x) if x == max_values[column] else \"{:.4f}\".format(x)\n",
    "        )\n",
    "    print(data.to_latex(escape=False, index=False))\n",
    "print_min_pts_for_dataset(\"cluto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min pts, complex9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_min_pts_for_dataset(\"complex9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min pts, letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_min_pts_for_dataset(\"letter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg calculations for epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_avg_num_calculations_for_datasets_for_epsilon():\n",
    "    data = frame[\n",
    "        (frame[\"algorithm\"] == \"tanimoto\")\n",
    "        & (frame[\"minPts\"] == 5)\n",
    "    ][[\"avg_num_calculations_of_similarity\", \"filtering\", \"dataset\", \"Eps\"]]\n",
    "    \n",
    "    datasets = [\"cluto\", \"complex9\", \"letter\"]\n",
    "    filtering = [\"none\", \"real\", \"zpn\"]\n",
    "    out = None\n",
    "    for d in datasets:\n",
    "        for f in filtering:\n",
    "            snippet = data[(data[\"filtering\"] == f) & (data[\"dataset\"] == d)][[\"Eps\", \"avg_num_calculations_of_similarity\"]]\n",
    "            snippet = snippet.rename(columns={\n",
    "                \"avg_num_calculations_of_similarity\": \"avg_{}_{}\".format(f, d)\n",
    "            })\n",
    "            if out is None:\n",
    "                out = snippet\n",
    "                \n",
    "                print(out.columns)\n",
    "            else:\n",
    "                out = pd.merge(\n",
    "                    out,\n",
    "                    snippet,\n",
    "                    on=\"Eps\"\n",
    "                )\n",
    "    data = out\n",
    "    data = data.sort_values(by=\"Eps\")[\n",
    "        [\"Eps\"] + [\n",
    "            \"avg_{}_{}\".format(f, d) for d in datasets for f in filtering\n",
    "        ]\n",
    "    ]\n",
    "    min_values = data.min()\n",
    "    print(len(data))\n",
    "    for column in data.columns[1:]:\n",
    "        data[column] = data[column].apply(\n",
    "            lambda x: \"\\textbf{{{:d}}}\".format(int(x)) if (x == min_values[column] and \"none\" not in column) else \"{:d}\".format(int(x))\n",
    "        )\n",
    "    print(data.to_latex(escape=False, index=False))\n",
    "print_avg_num_calculations_for_datasets_for_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg time for epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time_for_datasets_for_epsilon():\n",
    "    data = frame[\n",
    "        (frame[\"algorithm\"] == \"tanimoto\")\n",
    "        & (frame[\"minPts\"] == 5)\n",
    "    ][[\"model_fitting\", \"filtering\", \"dataset\", \"Eps\"]]\n",
    "    \n",
    "    datasets = [\"cluto\", \"complex9\", \"letter\"]\n",
    "    filtering = [\"none\", \"real\", \"zpn\"]\n",
    "    out = None\n",
    "    for d in datasets:\n",
    "        for f in filtering:\n",
    "            snippet = data[(data[\"filtering\"] == f) & (data[\"dataset\"] == d)][[\"Eps\", \"model_fitting\"]]\n",
    "            snippet = snippet.rename(columns={\n",
    "                \"model_fitting\": \"time_{}_{}\".format(f, d)\n",
    "            })\n",
    "            if out is None:\n",
    "                out = snippet\n",
    "                \n",
    "                print(out.columns)\n",
    "            else:\n",
    "                out = pd.merge(\n",
    "                    out,\n",
    "                    snippet,\n",
    "                    on=\"Eps\"\n",
    "                )\n",
    "    data = out\n",
    "    data = data.sort_values(by=\"Eps\")[\n",
    "        [\"Eps\"] + [\n",
    "            \"time_{}_{}\".format(f, d) for d in datasets for f in filtering\n",
    "        ]\n",
    "    ]\n",
    "    min_values = data.min()\n",
    "    print(len(data))\n",
    "    for column in data.columns[1:]:\n",
    "        data[column] = data[column].apply(\n",
    "            lambda x: \"\\textbf{{{:d}}}\".format(int(x)) if (x == min_values[column] and \"none\" not in column) else \"{:d}\".format(int(x))\n",
    "        )\n",
    "    print(data.to_latex(escape=False, index=False))\n",
    "print_time_for_datasets_for_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time_correlations():\n",
    "    data = frame[\n",
    "        (frame[\"algorithm\"] == \"tanimoto\")\n",
    "        & (frame[\"minPts\"] == 5)\n",
    "    ][[\"model_fitting\", \"filtering\", \"dataset\", \"Eps\"]]\n",
    "    \n",
    "    datasets = [\"cluto\", \"complex9\", \"letter\"]\n",
    "    filtering = [\"none\", \"real\", \"zpn\"]\n",
    "    out = None\n",
    "    for d in datasets:\n",
    "        for f in filtering:\n",
    "            snippet = data[(data[\"filtering\"] == f) & (data[\"dataset\"] == d)][[\"Eps\", \"model_fitting\"]]\n",
    "            snippet = snippet.rename(columns={\n",
    "                \"model_fitting\": \"time_{}_{}\".format(f, d)\n",
    "            })\n",
    "            if out is None:\n",
    "                out = snippet\n",
    "            else:\n",
    "                out = pd.merge(\n",
    "                    out,\n",
    "                    snippet,\n",
    "                    on=\"Eps\"\n",
    "                )\n",
    "    data = out\n",
    "    data = data.sort_values(by=\"Eps\")[\n",
    "        [\"Eps\"] + [\n",
    "            \"time_{}_{}\".format(f, d) for d in datasets for f in filtering\n",
    "        ]\n",
    "    ]\n",
    "    times_data = data \n",
    "    \n",
    "    data = frame[\n",
    "        (frame[\"algorithm\"] == \"tanimoto\")\n",
    "        & (frame[\"minPts\"] == 5)\n",
    "    ][[\"avg_num_calculations_of_similarity\", \"filtering\", \"dataset\", \"Eps\"]]\n",
    "    \n",
    "    datasets = [\"cluto\", \"complex9\", \"letter\"]\n",
    "    filtering = [\"none\", \"real\", \"zpn\"]\n",
    "    out = None\n",
    "    for d in datasets:\n",
    "        for f in filtering:\n",
    "            snippet = data[(data[\"filtering\"] == f) & (data[\"dataset\"] == d)][[\"Eps\", \"avg_num_calculations_of_similarity\"]]\n",
    "            snippet = snippet.rename(columns={\n",
    "                \"avg_num_calculations_of_similarity\": \"avg_{}_{}\".format(f, d)\n",
    "            })\n",
    "            if out is None:\n",
    "                out = snippet\n",
    "            else:\n",
    "                out = pd.merge(\n",
    "                    out,\n",
    "                    snippet,\n",
    "                    on=\"Eps\"\n",
    "                )\n",
    "    data = out\n",
    "    data = data.sort_values(by=\"Eps\")[\n",
    "        [\"Eps\"] + [\n",
    "            \"avg_{}_{}\".format(f, d) for d in datasets for f in filtering\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    correlations_data = data\n",
    "    output_data = {}\n",
    "    for column_a, column_b in zip(times_data.columns[1:], correlations_data.columns[1:]):\n",
    "        print(column_a, column_b)\n",
    "        correlation = sp_stats.pearsonr(times_data[column_a], correlations_data[column_b])[0]\n",
    "        output_data[\"_\".join(column_a.split(\"_\")[1:])] = [\"{0:.4f}\".format(correlation)]\n",
    "    print(output_data)\n",
    "    output_data = pd.DataFrame(data=output_data)\n",
    "    print(output_data.to_latex(escape=False, index=False))\n",
    "    \n",
    "print_time_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real vec len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1\n",
    "fun = lambda x: 0.5 * ((1 + 1 / x) + np.sqrt((1 + 1 / x) ** 2 - 4))\n",
    "x = np.linspace(0.1, 1.0, num=1000)\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, 1 / fun(x) * length, label=r\"$$\\frac{1}{\\alpha}|u|$$\")\n",
    "plt.plot(x, fun(x) * length, label=r\"$$\\alpha|u|$$\")\n",
    "plt.xlabel(r\"$\\varepsilon$\")\n",
    "plt.legend()\n",
    "\n",
    "fig.savefig(\"../experiments/realveclen.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZPN vec len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 2\n",
    "fun = lambda x: x\n",
    "x = np.linspace(0.1, 1.0, num=1000)\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, fun(x) * length, label=r\"$\\varepsilon |u|$\")\n",
    "plt.plot(x, 1 / fun(x) * length, label=r\"$\\frac{1}{\\varepsilon} |u|$\")\n",
    "plt.xlabel(r\"$\\varepsilon$\")\n",
    "plt.legend()\n",
    "\n",
    "fig.savefig(\"../experiments/zpnveclen.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BORDER_POINT = \"Border\"\n",
    "NOISE_POINT = \"Noise\"\n",
    "MULTIPLE_CLUSTERS = \"Multiple Clusters\"\n",
    "\n",
    "\n",
    "def read_data(path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    with open(path) as f:\n",
    "        lines = [line for line in f.read().split(\"\\n\") if len(line) > 0]\n",
    "    coords = []\n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        components = line.split(\",\")\n",
    "        x, y = float(components[1]), float(components[2])\n",
    "        point_type = int(components[4])\n",
    "        assignments = components[5:]\n",
    "        if point_type == -1:\n",
    "            labels.append(NOISE_POINT)\n",
    "        elif point_type == 0 and len(assignments) > 1:\n",
    "            labels.append(MULTIPLE_CLUSTERS)\n",
    "        elif point_type == 0 and len(assignments) == 1:\n",
    "            labels.append(BORDER_POINT)\n",
    "        else:\n",
    "            labels.append(str(assignments[-1]))\n",
    "\n",
    "        coords.append((x, y))\n",
    "\n",
    "    return np.array(coords), np.array(labels)\n",
    "\n",
    "\n",
    "def visualize(path: str, legend=True):\n",
    "    x, y = read_data(path)\n",
    "    colormap = cm.get_cmap(\"tab10\")\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    print(\"Num multiple border points: {}\".format(get_num_border_points_with_multiple_assignments(Path(path))))\n",
    "    \n",
    "    for label in natural_sort(np.unique(y)):\n",
    "        ix = np.where(y == str(label))[0]\n",
    "        ax.scatter(x[ix, 0], x[ix, 1], s=15, label=label, cmap=colormap)\n",
    "    if legend:\n",
    "        ax.legend()\n",
    "    return fig\n",
    "\n",
    "fig = visualize(\"../experiments/sanity_experiment_tanimoto/OUT\")\n",
    "fig.savefig(\"../experiments/sanity_tanimoto.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize(\"../experiments/sanity_experiment_base/OUT\")\n",
    "fig.savefig(\"../experiments/sanity_base.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize(\"../experiments/sanity_experiment_tanimoto_zpn/OUT\")\n",
    "fig.savefig(\"../experiments/sanity_tanimoto_zpn.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize(\"../experiments/simpler_sanity_experiment_tanimoto/OUT\")\n",
    "fig.savefig(\"../experiments/simpler_sanity_tanimoto.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize(\"../experiments/simpler_sanity_experiment_base/OUT\")\n",
    "fig.savefig(\"../experiments/simpler_sanity_base.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize(\"../experiments/complex9_0.999_10_tanimoto_0_none/OUT\", legend=False)\n",
    "fig.legend(bbox_to_anchor=(0.9, 0.9), loc='upper left')\n",
    "fig.savefig(\"../experiments/complex9.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = 50\n",
    "pixels = 512\n",
    "\n",
    "def visualize_tanimoto(num: int = 10):\n",
    "    values_range = np.linspace(0.001, max_value, num=pixels)\n",
    "    fig, ax = plt.subplots(1, num, sharey=True, figsize=(num * 1.5, 2))\n",
    "    u_lengths, v_lengths = np.meshgrid(\n",
    "        values_range, values_range\n",
    "    )\n",
    "    \n",
    "    dots = u_lengths * v_lengths\n",
    "    measure = dots / (u_lengths ** 2 + v_lengths ** 2 - dots)\n",
    "    \n",
    "    v_min = measure.min()\n",
    "    v_max = measure.max()\n",
    "    \n",
    "    for i, angle in enumerate(np.linspace(0.0, math.pi / 2, num=num)):\n",
    "        dots = u_lengths * v_lengths * np.cos(angle)\n",
    "        measure = dots / (u_lengths ** 2 + v_lengths ** 2 - dots)\n",
    "        im = ax[i].imshow(\n",
    "            measure, \n",
    "            cmap=cm.jet,\n",
    "            vmin=v_min, \n",
    "            vmax=v_max,\n",
    "            extent=[0.001, max_value, 0.001, max_value],\n",
    "            origin=\"lower\",\n",
    "        )\n",
    "        ax[i].set_title(r\"$\\angle = {:.3f}$\".format(angle))\n",
    "        ax[i].set_xlabel(r\"$|u|$\")\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel(r\"$|v|$\")\n",
    "            \n",
    "        if i == num - 1:\n",
    "            cb = fig.colorbar(im, ax=ax[i])\n",
    "            cb.set_label(r\"$\\texttt{Tanimoto measure}$\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = visualize_tanimoto(8)\n",
    "fig.savefig(\"../experiments/tanimoto.pdf\", bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_folders = [\n",
    "    \"cluto-t7-10k_0.99_100_tanimoto_0_real\",\n",
    "    \"complex9_0.99_100_tanimoto_0_real\",\n",
    "    \"letter_0.99_25_tanimoto_0_real\",\n",
    "]\n",
    "final_data = []\n",
    "for file in tqdm.tqdm(files):\n",
    "    if file.parent.name not in final_folders:\n",
    "        continue\n",
    "    with open(file) as f:\n",
    "        dataset, eps, min_pts, algo, _, filtering = file.parent.name.split(\"_\")\n",
    "        \n",
    "        sample = yaml.safe_load(f.read().replace(\"\\t\", \"\"))\n",
    "        \n",
    "        sample.update({\n",
    "            \"dataset\": dataset if \"cluto\" not in dataset else \"cluto\",\n",
    "            \"algorithm\": algo,\n",
    "            \"filtering\": filtering,\n",
    "            \"model_fitting\": sample[\"-model_fitting\"],\n",
    "            \"data_reading\": sample[\"-data_reading\"],\n",
    "            \"davis_bouldin\": float(sample[\"davis_bouldin\"]),\n",
    "        })\n",
    "        \n",
    "        class_assignments = get_class_assignments(file.parent / \"OUT\")\n",
    "        total_border_points = seq(class_assignments).filter(lambda x: len(x) > 1).map(len).sum()\n",
    "        \n",
    "        sample[\"\\#Multi Borders\"] = total_border_points\n",
    "        \n",
    "        \n",
    "        del sample[\"-model_fitting\"]\n",
    "        del sample[\"-data_reading\"]\n",
    "        del sample[\"runtimes_miliseconds\"]\n",
    "        del sample[\"input_file\"]\n",
    "        final_data.append(sample)\n",
    "        \n",
    "        \n",
    "        \n",
    "final_frame = pd.DataFrame(final_data)\n",
    "final_frame = final_frame.rename(\n",
    "    columns={\n",
    "        'purity': 'Purity',\n",
    "        \"davis_bouldin\": \"Davis-Bouldin\",\n",
    "        \"rand\": \"RAND\",\n",
    "        \"silhouette\": \"Silhouette\"\n",
    "    }\n",
    ")\n",
    "final_frame = final_frame.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_final_results():\n",
    "    data = final_frame\n",
    "    data = data.sort_values(by=\"dataset\")[\n",
    "        [\"dataset\", \"minPts\", \"Eps\"] + metrics_columns + [\"model_fitting\", \"avg_num_calculations_of_similarity\"]\n",
    "    ]\n",
    "    max_values = data.max()\n",
    "    print(len(data))\n",
    "    for column in data.columns[1:]:\n",
    "        data[column] = data[column].apply(\n",
    "            lambda x: \"{:.4f}\".format(x)\n",
    "        )\n",
    "    print(data.to_latex(escape=False, index=False))\n",
    "print_final_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize(f\"../experiments/{final_folders[0]}/OUT\")\n",
    "fig.savefig(\"../experiments/final_cluto.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize(f\"../experiments/{final_folders[1]}/OUT\")\n",
    "fig.savefig(\"../experiments/final_complex9.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
